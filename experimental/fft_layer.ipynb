{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fast Fourier Transform (FFT) layer. Transforms the input into the frequency domain. The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super(FFTLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        complex_f = torch.fft.rfft(input=x, dim=-1, norm='forward')\n",
    "\n",
    "        # Concatenate the real and imaginary parts\n",
    "        return torch.cat((complex_f.real, complex_f.imag), dim=-1)\n",
    "\n",
    "\n",
    "class FFTEnrichLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Concatenate the input with the FFT of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super(FFTEnrichLayer, self).__init__()\n",
    "        self.fft = FFTLayer()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.cat((x, self.fft(x)), dim=-1)\n",
    "    \n",
    "\n",
    "class LinearFFTEnriched(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer that enriches the input with the FFT of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        super(LinearFFTEnriched, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fft_enrich = FFTEnrichLayer()\n",
    "\n",
    "        self.linear = nn.Linear(input_size + 2 * (input_size // 2 + 1), output_size)\n",
    "\n",
    "    def to(self, device: torch.device) -> Any:\n",
    "        self.linear.to(device)\n",
    "        return self\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.fft_enrich(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2342,  0.0018, -0.5272,  0.0961, -0.9642,  0.4963, -0.6998, -0.0798,\n",
       "          0.1426,  0.6610, -0.4360,  0.5705, -0.5102, -0.5341,  0.1643, -0.2726,\n",
       "          0.3506,  0.0122,  0.6797,  0.5360,  0.0830, -0.3847,  0.1212,  0.2409,\n",
       "          0.5764, -0.3908,  0.3484, -0.0890,  0.3017, -0.0988,  0.3992, -0.7167,\n",
       "         -0.0200,  0.3042, -0.2587, -0.1973, -0.5728, -0.4654,  0.0693, -0.4362,\n",
       "          0.4107, -0.4018, -0.2631,  0.5925,  0.5228,  0.2471,  0.1502, -0.1625,\n",
       "         -0.4511,  0.2259, -0.0824,  0.8160,  0.4593, -0.8728,  0.3651, -0.5238,\n",
       "         -0.1732, -0.2172, -0.0479, -0.3563, -0.0092,  0.4076,  0.1475,  0.7072,\n",
       "          0.1646, -0.4238,  0.3235, -0.4178, -0.2224, -0.0540,  0.6400, -0.3012,\n",
       "          0.2719, -0.3525,  0.9355,  0.2347,  0.8612, -0.3089,  0.2750,  0.6096,\n",
       "          0.4522,  0.3103,  0.0908,  0.0273,  0.3478,  0.2021, -0.8486,  1.1881,\n",
       "         -0.9351,  0.2825, -0.1407, -0.0525,  0.0139,  0.0466,  0.1369,  0.0904,\n",
       "         -0.1624,  0.3797,  0.3769, -0.3445,  0.2364, -0.9150,  0.0091, -0.4668,\n",
       "         -0.2942,  0.1199, -0.5722, -0.2701, -0.0727,  0.9186, -0.1813, -0.0441,\n",
       "         -0.0883,  0.0805,  0.0928, -0.1294,  0.4213,  0.0670, -0.3999,  0.4508,\n",
       "         -1.0180,  0.2203,  0.1637, -0.3767, -0.3217,  0.4643,  0.1578, -0.5310,\n",
       "         -0.4951, -0.4304,  0.1336,  0.0795, -0.0604,  0.2091,  0.4853,  0.6214,\n",
       "         -0.1630,  0.0357,  0.0157,  0.1610,  0.1013,  0.3199,  0.4374, -0.3203,\n",
       "         -0.5867,  0.5848, -0.2137,  0.3242, -0.2055, -0.4621, -0.0780,  0.0930,\n",
       "          0.0870,  0.1692,  0.7614,  0.2476, -0.0347, -0.0758,  0.6421,  0.4359,\n",
       "          0.4437,  0.0623, -0.3905,  0.1136,  0.2066,  0.1129,  0.1670,  0.2092,\n",
       "          0.3515, -0.2934,  0.3189, -0.2550, -0.0931, -0.4342, -0.2200],\n",
       "        [ 0.1585,  0.1901,  0.0091, -0.1117,  0.0786,  0.4805,  0.4053,  0.3834,\n",
       "          0.0584, -0.7945, -0.0057, -0.3530,  1.2861, -0.3009,  0.4828, -0.3476,\n",
       "          0.6022, -0.1027,  0.3468, -0.5753, -0.5294, -0.8514, -0.8701,  0.1639,\n",
       "          0.1818, -0.8762, -1.0689, -0.0733,  0.2173,  0.6008, -0.3538,  0.8688,\n",
       "          1.3684,  0.0152,  0.8379, -0.3185, -0.4810,  0.5542,  0.0617, -0.2050,\n",
       "          0.1904,  0.0670,  0.1994, -0.4439,  0.1459, -0.1311,  0.2129, -0.1993,\n",
       "          0.1280,  0.3482,  0.1364, -0.0079, -0.1699,  0.3715, -0.4105,  0.1644,\n",
       "         -0.2479,  0.3848,  0.6436, -0.4125,  0.5726, -1.3394,  0.5650,  0.1753,\n",
       "          0.3386, -0.1122,  0.1043,  1.0978, -0.3760,  0.7493,  0.2838,  0.0535,\n",
       "         -0.3139,  0.2487,  0.6239,  0.2751,  0.5181,  0.3131, -0.2635, -0.7390,\n",
       "          0.0848, -0.3992, -0.4308,  0.4285,  0.5788, -0.1662,  0.8124, -0.1563,\n",
       "          0.3055, -0.1911,  0.2117,  0.0142,  0.1807,  0.4213, -0.2798, -0.5817,\n",
       "         -0.0958, -0.3246, -0.4614,  0.0360, -0.0726,  0.3692, -0.0357,  0.6937,\n",
       "         -0.4238,  0.3153,  0.1305, -0.4905,  0.3688, -0.0817, -0.9538,  0.5214,\n",
       "         -0.3499, -0.4095,  0.2034, -0.1292, -0.4573, -0.0230,  0.7647,  0.3524,\n",
       "          0.2842,  0.4618, -0.0764,  0.1674,  0.0499, -0.1659, -0.2008, -0.2368,\n",
       "          0.0907,  0.2918, -0.5603,  0.0378,  0.0250, -0.2898,  0.2161, -0.4009,\n",
       "          0.1577,  1.0533, -0.2496,  0.4841, -0.1557,  0.6225,  0.3695, -0.3305,\n",
       "         -0.2443, -0.1932,  0.2588, -0.0348,  0.6822, -0.7074, -0.8514, -0.7077,\n",
       "          0.4194, -0.3427,  0.1342,  0.1229,  0.1423, -0.4158, -0.3856, -0.4098,\n",
       "         -0.1104,  0.4403,  0.5367, -0.2260,  0.0930,  0.6069, -0.3593,  0.1985,\n",
       "          0.2395,  0.6871, -0.1168,  0.0015,  0.0052, -0.4655, -0.0679]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearFFTEnriched(input_size=175, output_size=175)(torch.randn(2, 175))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcnf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
