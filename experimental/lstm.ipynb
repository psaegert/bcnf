{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float = 0.0, bidirectional: bool = False) -> None:\n",
    "        super(VerboseLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers - 1):\n",
    "            print(f\"Adding LSTM layer {i + 1} with input size {input_size} and hidden size {hidden_size} and dropout {dropout}\")\n",
    "            self.lstm.append(nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=bidirectional, batch_first=True))\n",
    "            input_size = hidden_size * (2 if bidirectional else 1)\n",
    "\n",
    "            if dropout > 0:\n",
    "                self.lstm.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        self.lstm.append(nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=bidirectional, batch_first=True))\n",
    "\n",
    "\n",
    "    def to(self, *args, **kwargs) -> 'VerboseLSTM':\n",
    "        self.lstm = self.lstm.to(*args, **kwargs)\n",
    "        self.linear = self.linear.to(*args, **kwargs)\n",
    "        return super().to(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = torch.empty(self.num_layers, x.size(0), x.size(1), self.hidden_size * (2 if self.bidirectional else 1), device=x.device)\n",
    "\n",
    "        lstm_index = 0\n",
    "        for layer in self.lstm:\n",
    "            if isinstance(layer, nn.LSTM):\n",
    "                print(x.shape)\n",
    "                x, _= layer.forward(x)\n",
    "                h[lstm_index] = x\n",
    "                lstm_index += 1\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        \n",
    "        return x,  h.permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bcnf.models import FullyConnectedFeatureNetwork\n",
    "from typing import Any, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualDomainLSTM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int,\n",
    "            fc_sizes: list[int],\n",
    "            fc_dropout: float = 0.0,\n",
    "            num_layers: int = 1,\n",
    "            dropout: float = 0.0,\n",
    "            bidirectional: bool = False,\n",
    "            pooling: str = 'mean') -> None:\n",
    "        super(DualDomainLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = fc_sizes[-1]\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.lstm = VerboseLSTM(input_size, hidden_size, num_layers, dropout, bidirectional)\n",
    "        self.frequency_lstm = VerboseLSTM(input_size, hidden_size, num_layers, dropout, bidirectional)\n",
    "\n",
    "        fc_sizes = [hidden_size * (2 if bidirectional else 1) * 2] + fc_sizes\n",
    "\n",
    "        self.fc = FullyConnectedFeatureNetwork(sizes=fc_sizes, dropout=fc_dropout)\n",
    "\n",
    "    def to(self, *args, **kwargs) -> 'VerboseLSTM':\n",
    "        self.lstm = self.lstm.to(*args, **kwargs)\n",
    "        self.linear = self.linear.to(*args, **kwargs)\n",
    "        return super().to(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_lstm, _ = self.lstm.forward(x)\n",
    "        \n",
    "        x_frequencies = torch.fft.rfft(x, dim=1)\n",
    "\n",
    "        x_frequencies_lstm, _ = self.frequency_lstm.forward(torch.cat([x_frequencies.real, x_frequencies.imag], dim=2))\n",
    "\n",
    "        if self.pooling == 'mean':\n",
    "            x_lstm_pooled = x_lstm.mean(dim=1)\n",
    "            x_frequencies_lstm_pooled = x_frequencies_lstm.mean(dim=1)\n",
    "        elif self.pooling == 'max':\n",
    "            x_lstm_pooled = x_lstm.max(dim=1).values\n",
    "            x_frequencies_lstm_pooled = x_frequencies_lstm.max(dim=1).values\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid pooling method: {self.pooling}\")\n",
    "        \n",
    "        x_cat = torch.cat([x_lstm_pooled, x_frequencies_lstm_pooled], dim=1)\n",
    "\n",
    "        return self.fc.forward(x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LSTM layer 1 with input size 3 and hidden size 13 and dropout 0.1\n",
      "Adding LSTM layer 2 with input size 26 and hidden size 13 and dropout 0.1\n",
      "Adding LSTM layer 3 with input size 26 and hidden size 13 and dropout 0.1\n",
      "Adding LSTM layer 4 with input size 26 and hidden size 13 and dropout 0.1\n",
      "Adding LSTM layer 1 with input size 3 and hidden size 13 and dropout 0.1\n",
      "Adding LSTM layer 2 with input size 26 and hidden size 13 and dropout 0.1\n",
      "Adding LSTM layer 3 with input size 26 and hidden size 13 and dropout 0.1\n",
      "Adding LSTM layer 4 with input size 26 and hidden size 13 and dropout 0.1\n"
     ]
    }
   ],
   "source": [
    "lstm = DualDomainLSTM(input_size=3, hidden_size=13, fc_sizes=[128], fc_dropout=0.5, num_layers=5, dropout=0.1, bidirectional=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcnf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
